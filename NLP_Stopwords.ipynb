{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Stopwords.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitin-barthwal/TextAnalytics/blob/master/NLP_Stopwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Fq6mX_ArzGIP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP :  Use Stopword\n",
        "\n",
        "**How  to use default Stopwords corpus present in Natural Language Toolkit (NLTK).**\n",
        "\n",
        "\n",
        "**Stopwords are the frequently occurring words in a text document. For example, a, the, is, are, etc**"
      ]
    },
    {
      "metadata": {
        "id": "p61mF3qb0i9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "aa814ab7-bd93-49e1-f39d-e6679ff8746a"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "y5MWuQ8o0Rly",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Loading stopwords Corpus**"
      ]
    },
    {
      "metadata": {
        "id": "fRGL3IlTzKjR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "E4XZli3YzKsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0db86706-adef-45cd-f29c-b5b5cfcd8159"
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        " \n",
        "#stopwords_files = [str(item) for item in stopwords.fileids()]\n",
        "#print (stopwords_files)\n",
        " \n",
        "print (stopwords.fileids())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'turkish']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gtu2PpRAzK0D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5f48b417-f8bf-467f-db5a-d35f28378147"
      },
      "cell_type": "code",
      "source": [
        "stopwords_english = [str(item) for item in stopwords.words('english')]\n",
        "\n",
        " \n",
        "print ('English Stopwords : ',stopwords.words('english'))\n",
        "print ('German Stopwords : ',stopwords.words('german'))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Stopwords :  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "German Stopwords :  ['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders', 'auch', 'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'daß', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', 'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'für', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin', 'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'können', 'könnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem', 'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern', 'sonst', 'über', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', 'vor', 'während', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'würde', 'würden', 'zu', 'zum', 'zur', 'zwar', 'zwischen']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kxxrr0cF1FOY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tokenize Words**\n",
        "\n",
        "We split the text sentence/paragraph into a list of words. Each word in the list is called a token."
      ]
    },
    {
      "metadata": {
        "id": "1URfdVd0zK3a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4ff68b14-7782-4399-c2fa-193b06cfb473"
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('punkt') \n",
        "text = \"Hello my name is Nitin Barthwal. I am a Machine Learning Student.\"\n",
        " \n",
        "# Normalize text\n",
        "# NLTK considers capital letters and small letters differently.\n",
        "# For example: Fox and fox are considered as two different words.\n",
        "# Hence, we convert all letters of our text into lowercase.\n",
        "text = text.lower()\n",
        " \n",
        "# tokenize text \n",
        "words = word_tokenize(text)\n",
        " \n",
        "print (words)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['hello', 'my', 'name', 'is', 'nitin', 'barthwal', '.', 'i', 'am', 'a', 'machine', 'learning', 'student', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6tleiPWs1dza",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Removing Punctuation**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RubURhLuzK6Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f71e9f94-2b90-4be0-dcaa-190e0319a5f5"
      },
      "cell_type": "code",
      "source": [
        "words = [w for w in words if w.isalpha()]\n",
        "print( words)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'my', 'name', 'is', 'nitin', 'barthwal', 'i', 'am', 'a', 'machine', 'learning', 'student']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w8Flypv614Ar",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Removing Stop Words**\n",
        "\n",
        "Here, we will remove stop words from our text data using the default stopwords corpus present in NLTK.\n",
        "\n",
        "Get list of English Stopwords"
      ]
    },
    {
      "metadata": {
        "id": "mjX89UHGzK9a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4597559b-f111-44ed-9fb6-2bd015aaca7f"
      },
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        " \n",
        "print ('Length of Stopwords',len(stop_words))\n",
        " \n",
        "words_filtered = words[:] # creating a copy of the words list\n",
        " \n",
        "for word in words:\n",
        "    if word in stop_words:        \n",
        "        words_filtered.remove(word)\n",
        " \n",
        "print (words_filtered)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Stopwords 179\n",
            "['hello', 'name', 'nitin', 'barthwal', 'machine', 'learning', 'student']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rfRRalgm2DM9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Updating Stop Words Corpus**\n",
        "\n",
        "Suppose, you don’t want to omit some stopwords for your text analysis. In such case, you have to remove those words from the stopwords list.\n",
        "\n",
        "Let’s suppose, you want the words over and under for your text analysis. The words “my”  are present in the stopwords corpus by default.\n",
        "Let’s remove them from the stopwords corpus."
      ]
    },
    {
      "metadata": {
        "id": "EQVH6AHu2BwQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "XAuxhy5R2CH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "063101e3-f6ec-4158-84b3-747c763befd3"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# set() function removes entries from the list . Here we removed 'my' from stopwords\n",
        "stop_words = set(stopwords.words('english')) - set(['my'])\n",
        "print ('Length of Stopwords',len(stop_words))\n",
        "print (stop_words)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Stopwords 178\n",
            "{'myself', \"that'll\", \"you'd\", 'an', 'once', 'you', 'too', 'very', 'will', 'these', 'having', 'couldn', 'just', 'through', 'it', 'am', \"haven't\", 'further', 'i', 'but', 'between', 'why', 'only', 'below', 'the', 'such', 't', 'll', 'was', 'each', 'some', 'mustn', 'and', 'hadn', 'again', 'off', 'can', 're', 'is', 'out', 'few', 'on', \"hadn't\", \"you're\", \"weren't\", 'been', 'most', 'now', 'not', 'mightn', 'don', \"didn't\", 'didn', \"shouldn't\", 'his', 'so', \"aren't\", \"you'll\", 'ours', 'into', 'which', 'with', 'we', 'of', 'were', 'because', 'until', \"don't\", 'should', 'yourself', 'that', 'doesn', \"it's\", 'their', 'ourselves', 'who', 'as', 'ain', 'him', \"couldn't\", \"wouldn't\", 'this', 'by', 'any', 'up', 'while', 'here', 'to', 's', 'where', 'after', 'do', 'during', 'when', 'there', 'more', 'yours', \"doesn't\", 'our', 'against', 'me', 'shan', 'its', 'ma', \"mustn't\", 'needn', \"isn't\", \"should've\", 'from', 'won', 'be', \"shan't\", 'down', 'than', 'weren', 'then', 'if', 'y', 'her', 'other', 'above', 'has', 'whom', 'does', 'herself', 'your', 'o', \"wasn't\", 'haven', 'all', 'he', \"mightn't\", 'are', \"won't\", 'them', 'in', 'd', 'hasn', 'under', 'wouldn', 'own', 'nor', 'wasn', 'themselves', 'itself', 'being', 'those', 'theirs', 'how', 've', \"needn't\", 'no', 'a', 'isn', 'yourselves', 'about', 'what', 'she', 'doing', 'for', 'm', \"you've\", 'over', 'have', 'they', \"hasn't\", 'same', 'before', 'hers', 'had', \"she's\", 'did', 'aren', 'both', 'himself', 'at', 'shouldn', 'or'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4ZqhopkQ2Rfj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ee9048c-ecbc-465a-c92f-c72fe816cd14"
      },
      "cell_type": "code",
      "source": [
        "words_filtered = words[:] # creating a copy of the words list\n",
        " \n",
        "for word in words:\n",
        "    if word in stop_words:        \n",
        "        words_filtered.remove(word)\n",
        " \n",
        "print (words_filtered)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'my', 'name', 'nitin', 'barthwal', 'machine', 'learning', 'student']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}